{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsioIz7JtOug",
        "outputId": "b8e774ef-f760-49ba-b1c8-405b98d45851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Define paths\n",
        "path_to_dataset = \"/content/drive/Shareddrives/Best Shared Drive Ever/EEG/DEAP-Dataset/Pre-processed/\"\n",
        "save_path = \"/content/drive/Shareddrives/Best Shared Drive Ever/EEG/DEAP-Dataset/Independent/\"\n",
        "\n",
        "# Initialize lists to store data and labels\n",
        "data_all = []\n",
        "label_all = []\n",
        "\n",
        "# Loop through each subject (01 to 32)\n",
        "for subject_id in range(1, 33):\n",
        "    subject_file = f\"{path_to_dataset}s{subject_id:02d}.npy\"\n",
        "    with open(subject_file, 'rb') as file:\n",
        "        sub = np.load(file, allow_pickle=True)\n",
        "        data_all.extend(sub[:,0])\n",
        "        label_all.extend(sub[:,1])\n",
        "\n",
        "# Convert data and labels to NumPy arrays\n",
        "data_all = np.array(data_all)\n",
        "label_all = np.array(label_all)\n",
        "\n",
        "# Column names for features\n",
        "n_features = data_all.shape[1]\n",
        "column_names = [f'C{i+1}B{j+1}' for i in range(n_features // 5) for j in range(5)]\n",
        "\n",
        "# Shuffle the data and labels together\n",
        "indices = list(range(len(data_all)))\n",
        "random.shuffle(indices)\n",
        "data_all = data_all[indices]\n",
        "label_all = label_all[indices]\n",
        "\n",
        "# Split the data (8 files for testing, rest for training)\n",
        "split_index = int(0.8 * len(data_all))  # 80% training, 20% testing\n",
        "data_training = data_all[:split_index]\n",
        "label_training = label_all[:split_index]\n",
        "data_testing = data_all[split_index:]\n",
        "label_testing = label_all[split_index:]\n",
        "\n",
        "# Creating DataFrames for training and testing sets\n",
        "train_df = pd.DataFrame(data_training, columns=column_names)\n",
        "train_df['Valence'], train_df['Arousal'], train_df['Domain'], train_df['Like'] = np.array(label_training).T\n",
        "\n",
        "test_column_names = [f'C{i+1}B{j+1}' for i in range(len(data_testing[0]) // 5) for j in range(5)]\n",
        "test_df = pd.DataFrame(data_testing, columns=test_column_names)\n",
        "test_df['Valence'], test_df['Arousal'], test_df['Domain'], test_df['Like'] = np.array(label_testing).T\n",
        "\n",
        "# Save DataFrames to CSV\n",
        "train_csv_path = os.path.join(save_path, \"train_data.csv\")\n",
        "test_csv_path = os.path.join(save_path, \"test_data.csv\")\n",
        "\n",
        "train_df.to_csv(train_csv_path, index=False)\n",
        "test_df.to_csv(test_csv_path, index=False)\n",
        "\n",
        "print(\"Training and testing datasets created and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01G_P6rzvFnJ",
        "outputId": "05b21923-e97f-4b70-c71d-a0cab980d8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and testing datasets created and saved successfully.\n"
          ]
        }
      ]
    }
  ]
}